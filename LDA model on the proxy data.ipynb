{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ed25880",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade gensim\n",
    "#pip install tqdm\n",
    "#pip install pyldavis\n",
    "# install pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e97d1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Define IAM role\n",
    "import boto3\n",
    "\n",
    "# NLP things\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# visualisation\n",
    "import pyLDAvis.gensim \n",
    "import pyLDAvis\n",
    "\n",
    "# import others\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import io\n",
    "from io import StringIO\n",
    "import string\n",
    "import re\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "import sys\n",
    "import urllib.parse\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25aca03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "# use English stopwords\n",
    "stops = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b13a1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "##  TEXT CLEANING FUNCTIONS\n",
    "#########################################\n",
    "\n",
    "# Function for deleting emoji\n",
    "# This function is from Adam (2018): https://stackoverflow.com/questions/33404752/removing-emojis-from-a-string-in-python\n",
    "def deleteEmojis(text):    \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "                              \"]+\", flags=re.UNICODE)    \n",
    "    return emoji_pattern.sub(r' ',text)\n",
    "\n",
    "# Function for deleting default tags or labels in the tweets like 'VIDEO:' and 'AUDIO:'\n",
    "# This function is partly from Bica (2010): https://stackoverflow.com/questions/33404752/removing-emojis-from-a-string-in-python\n",
    "def remove_tweet_marks(tweet):\n",
    "    tweet = re.sub('VIDEO:', '', tweet)  # remove 'VIDEO:' from start of tweet\n",
    "    tweet = re.sub('AUDIO:', '', tweet)  # remove 'AUDIO:' from start of tweet\n",
    "    tweet = re.sub('&amp', '', tweet)\n",
    "    tweet = re.sub('RT @', '', tweet) # keep one space\n",
    "    return tweet\n",
    "\n",
    "# Function for expanding contractions\n",
    "# This function is from Dubois (2017): https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "# Function for tweet text cleaning\n",
    "# This function is partly from Zx81 (2014): https://stackoverflow.com/questions/24399820/expression-to-remove-url-links-from-twitter-tweet/24399874\n",
    "# This function is partly from Oneporter (2014): https://stackoverflow.com/questions/817122/delete-digits-in-python-regex\n",
    "def text_cleaning(data):\n",
    "    # delete emoji\n",
    "    data = data.map(lambda text: deleteEmojis(text))\n",
    "    # deleting the URL\n",
    "    data = data.map(lambda text: re.sub(r\"http\\S+\", \"\", text))\n",
    "    # deleting 'VIDEO' and 'AUDIO'\n",
    "    data = data.map(lambda text: remove_tweet_marks(text))\n",
    "    # convert the relevant column to lowercase\n",
    "    data = data.str.lower()\n",
    "    # expending contractions\n",
    "    data = data.map(lambda text: decontracted(text))\n",
    "    # delete punctuations\n",
    "    data = data.map(lambda text: re.sub(r'[,\\.!?:;@#&*$¥+~•₹€£=—\\-\\–\\\\→\\⇢\\<\\>\\|\\“\\”\\’\\{\\}\\'\\\"\\`\\[\\]\\(\\)_\\-\\%\\/]', ' ', text))\n",
    "    # remove all single characters\n",
    "    data = data.map(lambda text: re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text))\n",
    "    # remove digits\n",
    "    # notice that in this case some product names or terms may contain numbers, e.g. \"P50\",\"4g\",\"5g\"\n",
    "    # Thus only remove those digits that are not part of another word\n",
    "    data = data.map(lambda text: re.sub(r'\\b\\d+\\b', ' ', text))\n",
    "    # deleting surplus spacings\n",
    "    data = data.map(lambda text:  re.sub(r'\\s+', ' ', text))\n",
    "    \n",
    "    # 删除过短的记录 delete short sentence?? # 可以结合word token，计算长度？\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1a60b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is for tokenising sentences in the corpus\n",
    "def tokenising_corpus(data):\n",
    "    # Transform df into list\n",
    "    words = data.tolist()\n",
    "    # tokenising each sentence\n",
    "    word_tokens = []\n",
    "    for tweet in words:\n",
    "        word_tokens.append(word_tokenize(tweet))\n",
    "        \n",
    "    return word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "456ecdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function allows user to remove stopwords\n",
    "# and also allow to specify and remove some irrelevant words in this case (such sentiments) for tuning the model\n",
    "# This will only apply to the first LDA model for get more clear topics\n",
    "def custom_words_remover(word_lst, text_tokens):\n",
    "    # create a new list with specified words removed \n",
    "    processed_tokens = []\n",
    "    for token in text_tokens:\n",
    "        processed_tokens.append([w for w in token if not w in word_lst])\n",
    "        \n",
    "    return processed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfbe50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is for stemming the words in the corpus\n",
    "def stemming_words(text_tokens):\n",
    "    #from nltk.stem import PorterStemmer\n",
    "    ps = PorterStemmer()\n",
    "    # stemming the tokens\n",
    "    stemmed = []\n",
    "    for token in text_tokens:\n",
    "        stemmed.append([ps.stem(word) for word in token])\n",
    "    \n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15ddf54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for making biagram\n",
    "# This funciton is from Prabhakaran (2018): https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
    "def make_bigrams(data,min_count,thres):\n",
    "    # Build the bigram model with min_count=10\n",
    "    # higher threshold fewer phrases.\n",
    "    bigram = gensim.models.Phrases(data, min_count=min_count, threshold=thres)\n",
    "    \n",
    "    # Faster way to get a sentence clubbed as a bigram\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    \n",
    "    return [bigram_mod[doc] for doc in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d47e18e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the perplexity and coherence score of the model\n",
    "# This function is from Kapadia (2019):https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0\n",
    "def model_benchmarking(data, model, dictionary, corpus):\n",
    "    # Compute Model Perplexity\n",
    "    p = model.log_perplexity(corpus)\n",
    "    print('\\nPerplexity: ', p)\n",
    "\n",
    "    # Compute Coherence Score\n",
    "    coherence_model_lda = CoherenceModel(model = model, \n",
    "                                         texts = data, \n",
    "                                         dictionary = dictionary, \n",
    "                                         coherence = 'c_v')\n",
    "\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "    print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccb928c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supporting function of the model tuning: build individual lda model and compute its coherence\n",
    "# This function is from Kapadia (2019):https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "def compute_coherence_values_basic(data,corpus,dictionary,k,alpha):\n",
    "    # build individual lda model\n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           alpha = alpha,\n",
    "                                           random_state=5,\n",
    "                                           passes=10)\n",
    "    \n",
    "    #p = lda_model.log_perplexity(corpus)\n",
    "    # build coherence model\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, \n",
    "                                         texts=data, \n",
    "                                         dictionary=dictionary, \n",
    "                                         coherence='c_v')\n",
    "    # get coherence score\n",
    "    return coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a22c982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function search the optimal hyperparameter settings for the lda model\n",
    "# Similar to the grid search \n",
    "# It can take a long time to run\n",
    "# This function is from Kapadia (2019):https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0\n",
    "def tuning_lda_model(data,corpus,dictiornay, min_topics, max_topics, step_size):\n",
    "    # tqdm is a progress bar for visualising the cost time\n",
    "    import tqdm\n",
    "\n",
    "    grid = {}\n",
    "    grid['Validation_Set'] = {}\n",
    "    \n",
    "    # Set topics range\n",
    "    topics_range = range(min_topics, max_topics, step_size)\n",
    "    \n",
    "    # Alpha parameter\n",
    "    alpha = [0.01, 0.1, 0.3, 0.6, 1]\n",
    "    alpha.append('symmetric')\n",
    "    alpha.append('asymmetric')\n",
    "    \n",
    "    # Use 75% of original corpus as the validation sets\n",
    "    num_of_docs = len(corpus)\n",
    "    corpus_sets = [ gensim.utils.ClippedCorpus(corpus, int(num_of_docs*0.75)),\n",
    "                    corpus]\n",
    "\n",
    "    corpus_title = ['75% Corpus', '100% Corpus']\n",
    "    model_results = {'Validation_Set': [],\n",
    "                     'Alpha':[],\n",
    "                     'Topics': [],\n",
    "                     'Coherence': []}\n",
    "    \n",
    "    # calculate iterating times\n",
    "    t = 0\n",
    "    for i in range(len(corpus_sets)):\n",
    "        for a in alpha:\n",
    "            for k in topics_range:\n",
    "                #print(i,' ',a,' ',k)\n",
    "                t += 1\n",
    "                \n",
    "    print('iteration times: ',t)\n",
    "    \n",
    "    # Can take a long time to run\n",
    "    pbar = tqdm.tqdm(total=t)\n",
    "    \n",
    "    # iterate through validation corpuses\n",
    "    for i in range(len(corpus_sets)):\n",
    "        # iterate through alpha values\n",
    "        for a in alpha:\n",
    "            # iterate through number of topics\n",
    "            for k in topics_range:\n",
    "                # Compute the coherence for each model\n",
    "                cv = compute_coherence_values_basic(data=data,\n",
    "                                                    corpus=corpus_sets[i],\n",
    "                                                    dictionary=dictiornay,\n",
    "                                                    k=k,\n",
    "                                                    alpha=a)                \n",
    "                # Save the model results\n",
    "                model_results['Validation_Set'].append(corpus_title[i])\n",
    "                model_results['Alpha'].append(a)\n",
    "                model_results['Topics'].append(k)\n",
    "                model_results['Coherence'].append(cv)\n",
    "                print('pass')\n",
    "                \n",
    "                pbar.update(1)\n",
    "                \n",
    "    pbar.close()\n",
    "    \n",
    "    return model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbaaefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This functions is for creating the documents-topic matrix\n",
    "# which can show the individual document's probabilities for each topic\n",
    "# This function is from Wang (2019): https://stackoverflow.com/questions/56408849/after-applying-gensim-lda-topic-modeling-how-to-get-documents-with-highest-prob\n",
    "\n",
    "def create_doc_topic_matrix(model, corpus, num_topics):\n",
    "    # Create a dictionary, with topic ID as the key, and the value is a list of tuples (docID, probability of this particular topic for the doc) \n",
    "    topic_dict = {i: [] for i in range(num_topics)}\n",
    "    \n",
    "    # Remember to set the minimum_probability=0 in the model or can't get probabilities of one under each topic\n",
    "    # Loop over all the documents to group the probability of each topic\n",
    "    for doc_id in range(len(corpus)):\n",
    "        topic_vector = model[corpus[doc_id]]\n",
    "        for topic_id, prob in topic_vector: \n",
    "            topic_dict[topic_id].append(prob)\n",
    "    \n",
    "    # Create documents-topic matrix\n",
    "    doc_topic = pd.DataFrame.from_dict(topic_dict)\n",
    "    \n",
    "    return doc_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "81c13600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for creating eta matrix for training the guided lda model\n",
    "# the eta matrix can be used as a prior belief on word probability\n",
    "# can be use to assign probabilities for each word-topic combination\n",
    "def create_eta_matrix(num_topics,top_n,lda_model,id2word):\n",
    "    # get dictionary length\n",
    "    dic_len = len(id2word.token2id)\n",
    "    # initialising eta matrix with 0.001\n",
    "    eta_matrix = np.full((num_topics, dic_len), 0.001)\n",
    "    \n",
    "    # update the eta_matrix\n",
    "    # add the confidence to top_n words based on the model output probabilities\n",
    "    # hierarchical assignment: assign top 10 words with extra 0.15 and assign the top 10-20 words with 0.1\n",
    "    for topic_i in range(num_topics):\n",
    "        top_words = lda_model.get_topic_terms(topicid=topic_i,topn=top_n)\n",
    "        #count = 0\n",
    "        for pair in top_words:\n",
    "            #print(pair[0],pair[1],'\\n')\n",
    "            if top_words.index(pair) < 10:\n",
    "                eta_matrix[topic_i][pair[0]] = pair[1] + 0.10\n",
    "            else:\n",
    "                eta_matrix[topic_i][pair[0]] = pair[1] + 0.05\n",
    "                \n",
    "    return eta_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9069203c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function is for deleting abandoned topic from the eta matrix\n",
    "# This function is from Deshpande (2012): https://stackoverflow.com/questions/3877491/deleting-rows-in-numpy-array\n",
    "def abandon_topic(topic_id, matrix):\n",
    "    matrix = np.delete(matrix, (topic_id), axis=0)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58998966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to model_path\n",
    "def save_lda_model(model, model_name, save_path):\n",
    "    # save the model to model_path\n",
    "    model.save(save_path+'{}.model'.format(model_name))\n",
    "    # get list of componenets\n",
    "    components = [file for file in os.listdir(model_path) if file.startswith(model_name)]\n",
    "    \n",
    "    return components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0404be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for uploading eta matrix and list of component to S3\n",
    "# This function is from Shabani (2018): https://stackoverflow.com/questions/49120069/writing-a-pickle-file-to-an-s3-bucket-in-aws\n",
    "def file_upload_helper(file, file_name, bucket_name):\n",
    "    # create S3 resource\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    \n",
    "    # covert the file to pkl\n",
    "    obj_pkl = pickle.dumps(file)\n",
    "    obj_key = '{}.pkl'.format(file_name)\n",
    "    \n",
    "    s3_resource.Object(bucket_name, obj_key).put(Body=obj_pkl)\n",
    "    print('Success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b041fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for uploading model to S3\n",
    "# This function is from Sophros (2020): https://stackoverflow.com/questions/61638940/save-a-gensim-lda-model-to-s3\n",
    "def model_upload_helper(file_lst, local_path, bucket_name):\n",
    "    for file_name in file_lst:\n",
    "        # get file path\n",
    "        file_path = local_path + file_name        \n",
    "        # create s3 resource\n",
    "        s3_resource = boto3.resource('s3')\n",
    "        # upload file\n",
    "        s3_resource.meta.client.upload_file(file_path, bucket_name, file_name)\n",
    "        print('successfully upload ' + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbde2709",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae32d33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2e803a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "babac71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "bucket_name = \"proxy-data-and-pre-collected-data-for-training\"\n",
    "file_key_text = \"20191226-reviews.csv\"\n",
    "file_key_brand = \"20191226-items.csv\"\n",
    "\n",
    "data_location_text = \"s3://proxy-data-and-pre-collected-data-for-training/20191226-reviews.csv\"\n",
    "data_location_brand = \"s3://proxy-data-and-pre-collected-data-for-training/20191226-items.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b625f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = pd.read_csv(data_location_text)\n",
    "df_brand = pd.read_csv(data_location_brand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fd99a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>name</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>verified</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>helpfulVotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B0000SX2UC</td>\n",
       "      <td>Janet</td>\n",
       "      <td>3</td>\n",
       "      <td>October 11, 2005</td>\n",
       "      <td>False</td>\n",
       "      <td>Def not best, but not worst</td>\n",
       "      <td>I had the Samsung A600 for awhile which is abs...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B0000SX2UC</td>\n",
       "      <td>Luke Wyatt</td>\n",
       "      <td>1</td>\n",
       "      <td>January 7, 2004</td>\n",
       "      <td>False</td>\n",
       "      <td>Text Messaging Doesn't Work</td>\n",
       "      <td>Due to a software issue between Nokia and Spri...</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B0000SX2UC</td>\n",
       "      <td>Brooke</td>\n",
       "      <td>5</td>\n",
       "      <td>December 30, 2003</td>\n",
       "      <td>False</td>\n",
       "      <td>Love This Phone</td>\n",
       "      <td>This is a great, reliable phone. I also purcha...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B0000SX2UC</td>\n",
       "      <td>amy m. teague</td>\n",
       "      <td>3</td>\n",
       "      <td>March 18, 2004</td>\n",
       "      <td>False</td>\n",
       "      <td>Love the Phone, BUT...!</td>\n",
       "      <td>I love the phone and all, because I really did...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B0000SX2UC</td>\n",
       "      <td>tristazbimmer</td>\n",
       "      <td>4</td>\n",
       "      <td>August 28, 2005</td>\n",
       "      <td>False</td>\n",
       "      <td>Great phone service and options, lousy case!</td>\n",
       "      <td>The phone has been great for every purpose it ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin           name  rating               date  verified  \\\n",
       "0  B0000SX2UC          Janet       3   October 11, 2005     False   \n",
       "1  B0000SX2UC     Luke Wyatt       1    January 7, 2004     False   \n",
       "2  B0000SX2UC         Brooke       5  December 30, 2003     False   \n",
       "3  B0000SX2UC  amy m. teague       3     March 18, 2004     False   \n",
       "4  B0000SX2UC  tristazbimmer       4    August 28, 2005     False   \n",
       "\n",
       "                                          title  \\\n",
       "0                   Def not best, but not worst   \n",
       "1                   Text Messaging Doesn't Work   \n",
       "2                               Love This Phone   \n",
       "3                       Love the Phone, BUT...!   \n",
       "4  Great phone service and options, lousy case!   \n",
       "\n",
       "                                                body  helpfulVotes  \n",
       "0  I had the Samsung A600 for awhile which is abs...           1.0  \n",
       "1  Due to a software issue between Nokia and Spri...          17.0  \n",
       "2  This is a great, reliable phone. I also purcha...           5.0  \n",
       "3  I love the phone and all, because I really did...           1.0  \n",
       "4  The phone has been great for every purpose it ...           1.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d07d6618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>helpfulVotes</th>\n",
       "      <th>brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Def not best, but not worst</td>\n",
       "      <td>I had the Samsung A600 for awhile which is abs...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Text Messaging Doesn't Work</td>\n",
       "      <td>Due to a software issue between Nokia and Spri...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Love This Phone</td>\n",
       "      <td>This is a great, reliable phone. I also purcha...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Love the Phone, BUT...!</td>\n",
       "      <td>I love the phone and all, because I really did...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Great phone service and options, lousy case!</td>\n",
       "      <td>The phone has been great for every purpose it ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                         title  \\\n",
       "0       3                   Def not best, but not worst   \n",
       "1       1                   Text Messaging Doesn't Work   \n",
       "2       5                               Love This Phone   \n",
       "3       3                       Love the Phone, BUT...!   \n",
       "4       4  Great phone service and options, lousy case!   \n",
       "\n",
       "                                                body  helpfulVotes brand  \n",
       "0  I had the Samsung A600 for awhile which is abs...           1.0   NaN  \n",
       "1  Due to a software issue between Nokia and Spri...          17.0   NaN  \n",
       "2  This is a great, reliable phone. I also purcha...           5.0   NaN  \n",
       "3  I love the phone and all, because I really did...           1.0   NaN  \n",
       "4  The phone has been great for every purpose it ...           1.0   NaN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge the two dataset\n",
    "df_brand = df_brand[['asin','brand']]\n",
    "df = df_text.merge(df_brand, how='left', on='asin')\n",
    "#drop irrelevant contents\n",
    "df.drop(['asin','name','date','verified'], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "717ee091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>helpfulVotes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ASUS</th>\n",
       "      <td>251</td>\n",
       "      <td>251</td>\n",
       "      <td>251</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Apple</th>\n",
       "      <td>5145</td>\n",
       "      <td>5144</td>\n",
       "      <td>5145</td>\n",
       "      <td>1792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Google</th>\n",
       "      <td>3787</td>\n",
       "      <td>3787</td>\n",
       "      <td>3786</td>\n",
       "      <td>1743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HUAWEI</th>\n",
       "      <td>2225</td>\n",
       "      <td>2225</td>\n",
       "      <td>2225</td>\n",
       "      <td>1006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Motorola</th>\n",
       "      <td>8880</td>\n",
       "      <td>8880</td>\n",
       "      <td>8879</td>\n",
       "      <td>3686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nokia</th>\n",
       "      <td>5915</td>\n",
       "      <td>5915</td>\n",
       "      <td>5914</td>\n",
       "      <td>2750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OnePlus</th>\n",
       "      <td>347</td>\n",
       "      <td>347</td>\n",
       "      <td>347</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Samsung</th>\n",
       "      <td>33629</td>\n",
       "      <td>33616</td>\n",
       "      <td>33612</td>\n",
       "      <td>12567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sony</th>\n",
       "      <td>3196</td>\n",
       "      <td>3196</td>\n",
       "      <td>3195</td>\n",
       "      <td>1676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Xiaomi</th>\n",
       "      <td>4411</td>\n",
       "      <td>4411</td>\n",
       "      <td>4411</td>\n",
       "      <td>1557</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          rating  title   body  helpfulVotes\n",
       "brand                                       \n",
       "ASUS         251    251    251           132\n",
       "Apple       5145   5144   5145          1792\n",
       "Google      3787   3787   3786          1743\n",
       "HUAWEI      2225   2225   2225          1006\n",
       "Motorola    8880   8880   8879          3686\n",
       "Nokia       5915   5915   5914          2750\n",
       "OnePlus      347    347    347           171\n",
       "Samsung    33629  33616  33612         12567\n",
       "Sony        3196   3196   3195          1676\n",
       "Xiaomi      4411   4411   4411          1557"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initial exploratory data analysis\n",
    "df.groupby(['brand']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5eda1987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rating              0\n",
      "title              14\n",
      "body               21\n",
      "helpfulVotes    40771\n",
      "brand             200\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check NA\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0310bb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete NA in review title and body\n",
    "df = df.dropna(axis=0, subset=['title','body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65d6b450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the dataframe index\n",
    "df = df.reset_index()\n",
    "# drop old index column\n",
    "df = df.drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3e0941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the review title and body into full text for analysis\n",
    "df['text'] = df['title'] + ' ' +df['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fa56db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>helpfulVotes</th>\n",
       "      <th>brand</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Def not best, but not worst</td>\n",
       "      <td>I had the Samsung A600 for awhile which is abs...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Def not best, but not worst I had the Samsung ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Text Messaging Doesn't Work</td>\n",
       "      <td>Due to a software issue between Nokia and Spri...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Text Messaging Doesn't Work Due to a software ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Love This Phone</td>\n",
       "      <td>This is a great, reliable phone. I also purcha...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Love This Phone This is a great, reliable phon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Love the Phone, BUT...!</td>\n",
       "      <td>I love the phone and all, because I really did...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Love the Phone, BUT...! I love the phone and a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Great phone service and options, lousy case!</td>\n",
       "      <td>The phone has been great for every purpose it ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Great phone service and options, lousy case! T...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                         title  \\\n",
       "0       3                   Def not best, but not worst   \n",
       "1       1                   Text Messaging Doesn't Work   \n",
       "2       5                               Love This Phone   \n",
       "3       3                       Love the Phone, BUT...!   \n",
       "4       4  Great phone service and options, lousy case!   \n",
       "\n",
       "                                                body  helpfulVotes brand  \\\n",
       "0  I had the Samsung A600 for awhile which is abs...           1.0   NaN   \n",
       "1  Due to a software issue between Nokia and Spri...          17.0   NaN   \n",
       "2  This is a great, reliable phone. I also purcha...           5.0   NaN   \n",
       "3  I love the phone and all, because I really did...           1.0   NaN   \n",
       "4  The phone has been great for every purpose it ...           1.0   NaN   \n",
       "\n",
       "                                                text  \n",
       "0  Def not best, but not worst I had the Samsung ...  \n",
       "1  Text Messaging Doesn't Work Due to a software ...  \n",
       "2  Love This Phone This is a great, reliable phon...  \n",
       "3  Love the Phone, BUT...! I love the phone and a...  \n",
       "4  Great phone service and options, lousy case! T...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70f21d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "##  BASIC CLEANING AND TEXT PROCESSING\n",
    "#########################################\n",
    "\n",
    "# text cleaning\n",
    "df['text'] = text_cleaning(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "068f691f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenising sentences in the corpus\n",
    "word_tokens = tokenising_corpus(df['text'])\n",
    "\n",
    "# keep the copies\n",
    "df['text_tokens'] = word_tokens\n",
    "\n",
    "# get the length of each text\n",
    "df['text_len'] = df['text_tokens'].map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "602c2aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "#      Deleting stop words\n",
    "###################################\n",
    "\n",
    "# create a list of stopwords\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "# remove stopwords\n",
    "filtered_tokens = custom_words_remover(stops, word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681d8001",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "#           Stemming\n",
    "###################################\n",
    "stemmed = stemming_words(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b98bced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWhen keeping the emotional words, the LDA model tends to classify topics based on positive and negative sentiments, \\nrather than based on single functions. \\nFor example, the negative comments about the price and the negative comments about the battery\\nwill be put under one topic. In this way, reviews about one single function might be scattered on several topics. \\nHowever, the focus of the first LDA model should be extract hot words about specific cellphone features or functions \\nand generate the corresponding eta matrix.  \\nTherefore, when tuning the model, irrelevant words that express sentiments should be removed from the texts.\\n\\n\"star\" will also be deleted, \\nbecause in this case \"star\" is typically used to describe the sentimental polarity of customers (5 star = best while 1star = worst)\\n\\nAnother example is that in the first model, the topic \"screen\" might associate with strongly positive attitudes, \\nbut when it comes to the second model, most people might actually complain the srceen of the Huawei P50, \\nand associate the negetive word with the topic \"screen\"\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "When keeping the emotional words, the LDA model tends to classify topics based on positive and negative sentiments, \n",
    "rather than based on single functions. \n",
    "For example, the negative comments about the price and the negative comments about the battery\n",
    "will be put under one topic. In this way, reviews about one single function might be scattered on several topics. \n",
    "However, the focus of the first LDA model should be extract hot words about specific cellphone features or functions \n",
    "and generate the corresponding eta matrix.  \n",
    "Therefore, when tuning the model, irrelevant words that express sentiments should be removed from the texts.\n",
    "\n",
    "\"star\" will also be deleted, \n",
    "because in this case \"star\" is typically used to describe the sentimental polarity of customers (5 star = best while 1star = worst)\n",
    "\n",
    "Another example is that in the first model, the topic \"screen\" might associate with strongly positive attitudes, \n",
    "but when it comes to the second model, most people might actually complain the srceen of the Huawei P50, \n",
    "and associate the negetive word with the topic \"screen\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4a79c006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use defined function to specify some irrelevant words in this case (such sentiments) for tuning the model\n",
    "# This will only apply to the first LDA model for get more clear topics\n",
    "\n",
    "# Add emotional words as new stopword list\n",
    "newstopwords = ['like', 'good', 'better', 'best', 'bad', 'worse', 'worst', 'happi', 'great', 'really','realli',\n",
    "                'love', 'lov', 'also', 'awesome','awesom','amaz','lousi','far','well','perfectli','ok',\n",
    "                'ever','perfect','fun','excelent','excel','excelled','absolut','less','much','more','fewer','fine','finest',\n",
    "                'exactli','poor','pleas','glad','veri','high','terribl','minim','never','even','thank','gift','star','thank'] \n",
    "\n",
    "# delete new added stopwords from the text tokens\n",
    "df['processed_tokens'] = custom_words_remover(newstopwords, stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "441fd355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phrase Modeling: Making Bigrams\n",
    "# Build the bigram model with min_count=10\n",
    "# higher threshold fewer phrases.\n",
    "df['processed_tokens'] = make_bigrams(data= df['processed_tokens'],min_count= 10,thres= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "70bc9ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop processed columns\n",
    "df.drop(['title','body'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d48b0bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>helpfulVotes</th>\n",
       "      <th>brand</th>\n",
       "      <th>text</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>text_len</th>\n",
       "      <th>processed_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>def not best but not worst had the samsung a60...</td>\n",
       "      <td>[def, not, best, but, not, worst, had, the, sa...</td>\n",
       "      <td>313</td>\n",
       "      <td>[def, samsung, a600, awhil, doo, doo, read, re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>17.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>text messaging does not work due to software i...</td>\n",
       "      <td>[text, messaging, does, not, work, due, to, so...</td>\n",
       "      <td>136</td>\n",
       "      <td>[text, messag, work, due, softwar, issu, nokia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>love this phone this is great reliable phone a...</td>\n",
       "      <td>[love, this, phone, this, is, great, reliable,...</td>\n",
       "      <td>126</td>\n",
       "      <td>[phone, reliabl, phone, purchas, phone, samsun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>love the phone but love the phone and all beca...</td>\n",
       "      <td>[love, the, phone, but, love, the, phone, and,...</td>\n",
       "      <td>101</td>\n",
       "      <td>[phone, phone, need, one, expect, price, bill,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>great phone service and options lousy case the...</td>\n",
       "      <td>[great, phone, service, and, options, lousy, c...</td>\n",
       "      <td>132</td>\n",
       "      <td>[phone, servic, option, case, phone, everi, pu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating  helpfulVotes brand  \\\n",
       "0       3           1.0   NaN   \n",
       "1       1          17.0   NaN   \n",
       "2       5           5.0   NaN   \n",
       "3       3           1.0   NaN   \n",
       "4       4           1.0   NaN   \n",
       "\n",
       "                                                text  \\\n",
       "0  def not best but not worst had the samsung a60...   \n",
       "1  text messaging does not work due to software i...   \n",
       "2  love this phone this is great reliable phone a...   \n",
       "3  love the phone but love the phone and all beca...   \n",
       "4  great phone service and options lousy case the...   \n",
       "\n",
       "                                         text_tokens  text_len  \\\n",
       "0  [def, not, best, but, not, worst, had, the, sa...       313   \n",
       "1  [text, messaging, does, not, work, due, to, so...       136   \n",
       "2  [love, this, phone, this, is, great, reliable,...       126   \n",
       "3  [love, the, phone, but, love, the, phone, and,...       101   \n",
       "4  [great, phone, service, and, options, lousy, c...       132   \n",
       "\n",
       "                                    processed_tokens  \n",
       "0  [def, samsung, a600, awhil, doo, doo, read, re...  \n",
       "1  [text, messag, work, due, softwar, issu, nokia...  \n",
       "2  [phone, reliabl, phone, purchas, phone, samsun...  \n",
       "3  [phone, phone, need, one, expect, price, bill,...  \n",
       "4  [phone, servic, option, case, phone, everi, pu...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062079e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc6eb247",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "#  Create word dictionary and bag of words (bow) corpus\n",
    "#########################################################\n",
    "# Create id to word Dictionary\n",
    "# id2word is a dictionary containing the IDs of all input words\n",
    "id2word = corpora.Dictionary(df['processed_tokens'])\n",
    "\n",
    "# Create corpus that contains all documents\n",
    "texts = df['processed_tokens']\n",
    "\n",
    "# Create bag of word for each document in the corpus \n",
    "# each bow contains the id of each word in that single document and its number of occurrences in that document \n",
    "# (term id, term document Frequency)\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ad5add",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0286cd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "##            Building the basic LDA model\n",
    "##########################################################################\n",
    "# assuming number of topics\n",
    "num_topics = 10\n",
    "\n",
    "# Build LDA model\n",
    "# Remember to set the minimum_probability=0 in the model or can't get probabilities of a word under each topic\n",
    "lda_model = gensim.models.LdaMulticore(corpus = corpus,\n",
    "                                       id2word = id2word,\n",
    "                                       num_topics = num_topics,\n",
    "                                       passes = 10,\n",
    "                                       random_state=5, \n",
    "                                       minimum_probability=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "02ae2bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -7.131014341772236\n",
      "\n",
      "Coherence Score:  0.4913308492786602\n"
     ]
    }
   ],
   "source": [
    "# Compute the perplexity and coherence score of the model\n",
    "model_benchmarking(df['processed_tokens'], lda_model, id2word, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96515a85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d06042",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "##                  Tuning the LDA model on proxy data\n",
    "##########################################################################\n",
    "# Use defined functions to tune the lda model and find optimal hyperparameter settings\n",
    "# It can take a long time to run\n",
    "model_results = tuning_lda_model(data = df['processed_tokens'],\n",
    "                                 corpus = corpus,\n",
    "                                 dictiornay = id2word,\n",
    "                                 min_topics = 5, \n",
    "                                 max_topics = 6,\n",
    "                                 step_size = 1)\n",
    "\n",
    "# Convert to the dataframe and save to the csv files\n",
    "model_results_df = pd.DataFrame.from_dict(model_results)\n",
    "model_results_df.to_csv(\"lda_on_proxy_tuning_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9419cadc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1167fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b75d2bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/14 [23:15<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "##########################################################################\n",
    "##          Building the LDA model (with optimal parameter)\n",
    "##########################################################################\n",
    "\n",
    "# Optimal model after tuning: \n",
    "# Hyperparameters: num_topics = 11, alpha = 'symmetric', passes =10\n",
    "\n",
    "# Set number of topics\n",
    "num_topics = 11\n",
    "\n",
    "# Build LDA model\n",
    "# Remember to set the minimum_probability=0 in the model or can't get probabilities of a word under each topic\n",
    "lda_model = gensim.models.LdaMulticore(corpus = corpus,\n",
    "                                       id2word = id2word,\n",
    "                                       num_topics = num_topics,\n",
    "                                       passes = 10,\n",
    "                                       alpha = 'symmetric',\n",
    "                                       random_state=5, \n",
    "                                       minimum_probability=0)\n",
    "\n",
    "# Perplexity:  -7.206703803237903\n",
    "# Coherence Score:  0.5061053071793964"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f805afa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -7.206703803237903\n",
      "\n",
      "Coherence Score:  0.5061053071793964\n"
     ]
    }
   ],
   "source": [
    "# Compute the perplexity and coherence score of the model\n",
    "model_benchmarking(df['processed_tokens'], lda_model, id2word, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd88dce0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ce26bed8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.061*\"phone\" + 0.055*\"call\" + 0.032*\"work\" + 0.021*\"speaker\" + '\n",
      "  '0.021*\"issu\" + 0.020*\"wifi\" + 0.019*\"connect\" + 0.016*\"problem\" + '\n",
      "  '0.016*\"sound\" + 0.015*\"hear\" + 0.014*\"time\" + 0.011*\"volum\" + '\n",
      "  '0.011*\"bluetooth\" + 0.011*\"sometim\" + 0.010*\"para\" + 0.010*\"make\" + '\n",
      "  '0.010*\"freez\" + 0.010*\"signal\" + 0.009*\"slow\" + 0.009*\"precio\"'),\n",
      " (1,\n",
      "  '0.056*\"phone\" + 0.026*\"use\" + 0.015*\"text\" + 0.013*\"call\" + 0.010*\"get\" + '\n",
      "  '0.010*\"need\" + 0.010*\"want\" + 0.009*\"one\" + 0.008*\"go\" + 0.008*\"would\" + '\n",
      "  '0.008*\"featur\" + 0.008*\"time\" + 0.007*\"messag\" + 0.007*\"nokia\" + '\n",
      "  '0.007*\"make\" + 0.006*\"thing\" + 0.005*\"set\" + 0.005*\"window\" + 0.005*\"iphon\" '\n",
      "  '+ 0.005*\"work\"'),\n",
      " (2,\n",
      "  '0.168*\"batteri\" + 0.091*\"phone\" + 0.076*\"life\" + 0.056*\"five\" + '\n",
      "  '0.040*\"charg\" + 0.032*\"day\" + 0.030*\"last\" + 0.027*\"fast\" + 0.025*\"product\" '\n",
      "  '+ 0.022*\"work\" + 0.015*\"long\" + 0.013*\"four\" + 0.013*\"hour\" + 0.013*\"use\" + '\n",
      "  '0.012*\"expect\" + 0.009*\"time\" + 0.008*\"samsung\" + 0.007*\"nice\" + '\n",
      "  '0.007*\"galaxi\" + 0.007*\"drain\"'),\n",
      " (3,\n",
      "  '0.197*\"phone\" + 0.059*\"price\" + 0.031*\"work\" + 0.025*\"nice\" + 0.021*\"use\" + '\n",
      "  '0.019*\"camera\" + 0.019*\"qualiti\" + 0.015*\"valu\" + 0.015*\"money\" + '\n",
      "  '0.014*\"buy\" + 0.013*\"bought\" + 0.012*\"easi\" + 0.012*\"fast\" + '\n",
      "  '0.012*\"everyth\" + 0.011*\"one\" + 0.010*\"featur\" + 0.010*\"need\" + '\n",
      "  '0.010*\"worth\" + 0.009*\"recommend\" + 0.007*\"pictur\"'),\n",
      " (4,\n",
      "  '0.073*\"phone\" + 0.022*\"month\" + 0.021*\"work\" + 0.017*\"one\" + 0.016*\"charg\" '\n",
      "  '+ 0.015*\"would\" + 0.013*\"buy\" + 0.012*\"day\" + 0.012*\"back\" + 0.011*\"get\" + '\n",
      "  '0.011*\"issu\" + 0.011*\"replac\" + 0.010*\"return\" + 0.010*\"problem\" + '\n",
      "  '0.010*\"bought\" + 0.010*\"time\" + 0.009*\"week\" + 0.009*\"got\" + 0.008*\"stop\" + '\n",
      "  '0.008*\"warranti\"'),\n",
      " (5,\n",
      "  '0.040*\"app\" + 0.028*\"phone\" + 0.019*\"use\" + 0.015*\"updat\" + 0.012*\"card\" + '\n",
      "  '0.010*\"android\" + 0.009*\"get\" + 0.008*\"storag\" + 0.008*\"play\" + 0.008*\"sd\" '\n",
      "  '+ 0.007*\"memori\" + 0.007*\"set\" + 0.007*\"googl\" + 0.007*\"run\" + '\n",
      "  '0.006*\"instal\" + 0.006*\"fingerprint_reader\" + 0.006*\"work\" + 0.006*\"issu\" + '\n",
      "  '0.005*\"game\" + 0.005*\"download\"'),\n",
      " (6,\n",
      "  '0.047*\"el\" + 0.043*\"de\" + 0.032*\"es\" + 0.031*\"que\" + 0.028*\"la\" + '\n",
      "  '0.025*\"en\" + 0.024*\"lo\" + 0.020*\"con\" + 0.017*\"un\" + 0.016*\"teléfono\" + '\n",
      "  '0.014*\"se\" + 0.014*\"producto\" + 0.013*\"muy\" + 0.012*\"celular\" + 0.011*\"por\" '\n",
      "  '+ 0.011*\"mi\" + 0.010*\"pero\" + 0.009*\"todo\" + 0.009*\"muy_bueno\" + '\n",
      "  '0.008*\"muy_buen\"'),\n",
      " (7,\n",
      "  '0.074*\"phone\" + 0.063*\"new\" + 0.028*\"work\" + 0.028*\"came\" + 0.023*\"condit\" '\n",
      "  '+ 0.022*\"look\" + 0.022*\"brand\" + 0.021*\"charger\" + 0.018*\"scratch\" + '\n",
      "  '0.016*\"box\" + 0.014*\"arriv\" + 0.014*\"purchas\" + 0.013*\"buy\" + '\n",
      "  '0.013*\"refurbish\" + 0.013*\"product\" + 0.012*\"seller\" + 0.010*\"use\" + '\n",
      "  '0.010*\"receiv\" + 0.009*\"one\" + 0.009*\"would\"'),\n",
      " (8,\n",
      "  '0.080*\"phone\" + 0.029*\"work\" + 0.029*\"sim\" + 0.029*\"unlock\" + '\n",
      "  '0.023*\"verizon\" + 0.019*\"card\" + 0.015*\"mobil\" + 0.012*\"use\" + '\n",
      "  '0.012*\"servic\" + 0.011*\"network\" + 0.011*\"get\" + 0.011*\"carrier\" + '\n",
      "  '0.010*\"us\" + 0.010*\"samsung\" + 0.009*\"activ\" + 0.009*\"amazon\" + '\n",
      "  '0.008*\"could\" + 0.008*\"would\" + 0.007*\"buy\" + 0.007*\"version\"'),\n",
      " (9,\n",
      "  '0.027*\"camera\" + 0.027*\"phone\" + 0.015*\"android\" + 0.012*\"use\" + '\n",
      "  '0.011*\"qualiti\" + 0.011*\"devic\" + 0.010*\"iphon\" + 0.009*\"soni\" + '\n",
      "  '0.009*\"screen\" + 0.009*\"batteri\" + 0.007*\"look\" + 0.007*\"pixel\" + '\n",
      "  '0.007*\"perform\" + 0.006*\"feel\" + 0.006*\"one\" + 0.006*\"display\" + '\n",
      "  '0.006*\"googl\" + 0.006*\"featur\" + 0.006*\"video\" + 0.005*\"photo\"'),\n",
      " (10,\n",
      "  '0.051*\"phone\" + 0.043*\"screen\" + 0.016*\"note\" + 0.014*\"case\" + 0.013*\"get\" '\n",
      "  '+ 0.013*\"samsung\" + 0.011*\"use\" + 0.010*\"one\" + 0.009*\"galaxi\" + '\n",
      "  '0.008*\"back\" + 0.007*\"time\" + 0.007*\"would\" + 0.007*\"edg\" + 0.006*\"drop\" + '\n",
      "  '0.006*\"still\" + 0.006*\"thing\" + 0.006*\"go\" + 0.006*\"moto\" + 0.005*\"glass\" + '\n",
      "  '0.005*\"hand\"')]\n"
     ]
    }
   ],
   "source": [
    "# from pprint import pprint\n",
    "# print the top 20 keywords under each topic\n",
    "pprint(lda_model.print_topics(num_words=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab9d252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af85b563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "# Visualize the topics \n",
    "# lambda = 0.6 can be ideal\n",
    "pyLDAvis.enable_notebook()\n",
    "LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559423cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1f8de83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "##   Check topics and contents under each topic\n",
    "##########################################################\n",
    "# create test df\n",
    "test = df\n",
    "texts = test['processed_tokens']\n",
    "# create new corpus\n",
    "corpus_new = [lda_model.id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8981ee2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0         1         2         3         4         5         6   \\\n",
      "0  0.000612  0.284389  0.000612  0.070213  0.034841  0.000612  0.000612   \n",
      "1  0.001340  0.394747  0.001340  0.001340  0.130991  0.214136  0.001340   \n",
      "2  0.001526  0.465676  0.001526  0.146594  0.001526  0.242606  0.001526   \n",
      "3  0.002331  0.297373  0.101239  0.113062  0.002332  0.002332  0.002331   \n",
      "4  0.001653  0.001654  0.001654  0.001654  0.045375  0.001654  0.001653   \n",
      "\n",
      "         7         8         9         10  \n",
      "0  0.087441  0.059395  0.009382  0.451889  \n",
      "1  0.032093  0.208670  0.012665  0.001340  \n",
      "2  0.034764  0.001527  0.001527  0.101202  \n",
      "3  0.002332  0.472006  0.002331  0.002332  \n",
      "4  0.031937  0.306900  0.001654  0.604212  \n"
     ]
    }
   ],
   "source": [
    "# Creating the documents-topic matrix\n",
    "# which can show the individual document's probabilities for each topic\n",
    "doc_topic = create_doc_topic_matrix(model = lda_model,\n",
    "                                    corpus = corpus_new,\n",
    "                                    num_topics = num_topics)\n",
    "print(doc_topic.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "65f1a583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat documents-topic matrix with the review dataframe\n",
    "joined_df = pd.concat([df, doc_topic], axis = 1, join = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1a4605b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['not very rugged bought this phone because my previous one kept falling out of my belt clip and getting scratched up figured would pay the extra money to get mil spec phone and not have to worry about damage happening have kayak and wanted to be able to take phone out on the boat without worry about water getting in it also purchased verizon belt clip with holder that wraps around the phone well this first thing found was that the belt clip kept coming off my belt and the phone kept landing on hard floors etc the second thing know the outside screen thin plastic is cracked verizon wants minimum of to replace it with used phone unless would purchased insurance phone repair places can not get parts to repair it of course this eliminates the waterproofing feature since water can likely get in the cracks on the screen do not mind the cracks themselves since do not really use the outside screen third happening was when the holder that wraps around the phone started coming off and letting the phone fall out that is when discarded the belt clip holder and started carrying the phone in my pocket have decided was actually better off with the old phone would have had lot more dollars in my pocket since the mil spec designation adds nothing other than that it is pretty good phone just do not buy it for its ruggedness ',\n",
       " 'bad screen gets easily cracked used the phone for few weeks worked okay except that the phone needs to be rebooted once in while but the screen is the worst part of the phone and any minor fall of the phone would easily break the screen in my case was trying to keep the phone on our coffee table and it fell from just or 6in on an edge and the screen completely shattered even small pieces of glasses fell on my table had used iphone for several years and these sort of slippages never caused any damage to it worst quality phone and will never buy again from motorola ',\n",
       " 'this is my second of these phones it survives this is my second of these phones it survives the harshest owner do not fault the first one for giving up dropped it down flight of cement apartment stairs which it survived that cracked the outer case then it slipped out of my shirt pocket into full tub when went to kiss my kid on the head before going to work frankly no complaint prior to the case cracking it survived full submersion had to dry out the fact that ordered second says something ',\n",
       " 'frequent screen cracks before buying this phone research droid rzr cracked screens there are lots of negative reviews from new owners how insurance does not cover much on the average screen replacement my husb put it in his back pocket and then noticed it was cracked even in the otter box protective cover not good choice for child or someone who can not baby the phone our estimate to fix the screen is verizon and motorola just now acknowledging frequent screen cracks ',\n",
       " 'screen cracks be very careful the screen cracks way to easily had the phone in my shorts and must have bumped into something and when took my phone out the screen was cracked did not drop or sit on it gorilla glass is joke if walking and bumping into something cracks the screen mine is right at the home button it is smashed there and the crack goes half way up the phone ',\n",
       " 'back glass breaks to easily this phone was great except one critical flaw the back is unnecessaryly made out of glass when it broke wires were exposed and it was impossible to avoid getting cut making the entire phone useless do not buy phone with glass on the back it does nothing but add weak point ',\n",
       " 'screen and finger print reader smashed from dropping it two feet have good phone case and screen protector and dropped the phone while walking and it smashed have dropped other phones hundreds of times and no damage the screen repair cost from what can tell costs the same and the phone am very disappointed good phone other than not being durable',\n",
       " 'love this phone love this phone had one previously and when the screen finally cracked after multiple drops and no case or screen protector on it had to get another one forget all the fancy things on newer phones have not found any phone to match the durability of this one this time put screen protector on it at least ',\n",
       " 'the back of the phone is glass the phone is made of glass back and all the glass back the phone is like shattered from putting it in my back pocket why would phone have more glass on it than it needs it lasted about months the phone still works but would not advise anyone to buy it ',\n",
       " 'broken after days of use this is my second z4 the first one broke when it slipped from my hand on my desk and the screen cracked right away the second one slipped from my hand again on my wooden coach which cracked the screen completely althogut it has screen protecter on very fragile and not recommended',\n",
       " 'you cannot drop it had droid force which had gorilla glass face for three years no problem thought the z4 was the replacement had the same glass drop the phone in about ft on loose gravel not moving in the face shattered this is my fourth motorola phone and think my last',\n",
       " 'phone material is disappointingly fragile the phones features are great the only thing is that have had the phone for week and not dropped it once today the phone cracked when dropped it was sitting on the floor when it slipped from my hand less than foot up and the entire screen cracked this material that it is made from is terribly fragile ',\n",
       " 'the iphone is water resistance rating is pure garbage just drowned two phones in an inch the iphone is water resistance rating is pure garbage just drowned two phones in an inch of water within few minutes anyone know of class action lawsuit for this ',\n",
       " 'the iphone is water resistance rating is pure garbage just drowned two phones in an inch the iphone is water resistance rating is pure garbage just drowned two phones in an inch of water within few minutes anyone know of class action lawsuit for this ',\n",
       " 'awesome phone tough as nails and wate shock resistance love this phone work in very busy office and am always dropping my phone and getting it wet this phone is extremely durable and water resistant or in my case coffee resistant can not say enough about this pone love it ',\n",
       " 'if you have for screen repair buy it otherwise stay clear stay clear unless you have full armor case on it screen is very flimsy and would crack easily mine cracked in my pocket and repair quoted for from ubreakifix do not buy ',\n",
       " 'glad found another second one of these have owned with my line of work farming am really tough on phone previous barrage lasted years through all the abuse glad found another because simple function mil spec phones are hard to find ',\n",
       " 'extremely fragile screen screen cracked within the first month of me getting it from foot drop onto wooden counter screen repair costs more than getting new one would not recommend it to anyone ',\n",
       " 'extremely fragile screen screen cracked within the first month of me getting it from foot drop onto wooden counter screen repair costs more than getting new one would not recommend it to anyone ']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the 20 comments that are most relevant to topic n\n",
    "# Notice that the column name is INT value in this case\n",
    "joined_df.sort_values(by = 10,ascending=False)['text'].iloc[0:19].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "37623640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('phone', 0.11901073),\n",
       " ('work', 0.062967874),\n",
       " ('new', 0.06042277),\n",
       " ('came', 0.028426593),\n",
       " ('look', 0.026468102),\n",
       " ('condit', 0.025736837),\n",
       " ('brand', 0.023311382),\n",
       " ('price', 0.022512201),\n",
       " ('expect', 0.018078534),\n",
       " ('everyth', 0.017559929),\n",
       " ('scratch', 0.016999235),\n",
       " ('buy', 0.015849011),\n",
       " ('purchas', 0.01523436),\n",
       " ('arriv', 0.014811356),\n",
       " ('fast', 0.014788588),\n",
       " ('product', 0.012467896),\n",
       " ('seller', 0.010434202),\n",
       " ('charger', 0.010263215),\n",
       " ('ship', 0.008981515),\n",
       " ('describ', 0.008774676)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the top 20 words under each topic\n",
    "lda_model.show_topic(topicid = 0, topn = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63e928a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "21b83c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "#    Create eta matrix\n",
    "#################################\n",
    "# Creating eta matrix with top 20 words under each topic\n",
    "# the eta matrix can be used to train the guided lda model as a prior belief on word probability \n",
    "# can be use to assign probabilities for each word-topic combination\n",
    "eta_matrix = create_eta_matrix(num_topics,20,lda_model,id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "11e42ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 26345) \n",
      "\n",
      "[(1793, 0.03089669), (1410, 0.026323322), (116, 0.0187444), (72, 0.01663353), (1327, 0.016337896), (755, 0.014705432), (3577, 0.010032764), (158, 0.007873814), (367, 0.007668735), (619, 0.007611291), (43, 0.007158337), (401, 0.0068745613), (740, 0.006723654), (182, 0.0061422177), (185, 0.0057118465), (217, 0.0056817266), (788, 0.0056452076), (181, 0.0055978703), (1391, 0.0054754717), (783, 0.005114052)] \n",
      "\n",
      "0.11663352921605111 \n",
      "\n",
      "0.001 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if it works well\n",
    "print(eta_matrix.shape,'\\n')\n",
    "print(lda_model.get_topic_terms(topicid=5,topn=20),'\\n')\n",
    "print(eta_matrix[5][72],'\\n')\n",
    "print(eta_matrix[5][122],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eda9d9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting abandoned topic from the eta matrix\n",
    "# topic 3 is reviews in Spanish and topic 4 is talking about cellphone refurbishment\n",
    "# They can be considered irrelevant in the future analysis, so need to be abandoned\n",
    "# abandon topic 3: Spanish \n",
    "eta_matrix = abandon_topic(topic_id = 3, matrix = eta_matrix)\n",
    "# abandon topic 4: Refurbishment\n",
    "# Notice now original topic 4 become the topic 3 in the eta matrix (after deleting the previous topic3)\n",
    "eta_matrix = abandon_topic(topic_id = 3, matrix = eta_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0b8681bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 26345) \n",
      "\n",
      "[(72, 0.19704744), (191, 0.0590912), (122, 0.030578958), (61, 0.024904126), (116, 0.020562498), (367, 0.019361166), (341, 0.019017749), (314, 0.015218113), (146, 0.0146378195), (258, 0.014177452)] \n",
      "\n",
      "[(72, 0.07276104), (147, 0.021716094), (122, 0.021425875), (217, 0.016713087), (15, 0.015563478), (123, 0.014971952), (258, 0.013351448), (235, 0.012475495), (227, 0.01203926), (43, 0.011461665)] \n",
      "\n",
      "[(1793, 0.03986148), (72, 0.028285624), (116, 0.018964099), (755, 0.015180794), (618, 0.01181858), (1410, 0.0098934015), (43, 0.008738315), (1391, 0.008016017), (401, 0.0079249935), (2993, 0.0075179385)] \n",
      "\n",
      "0.17828562445938587 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if it works well\n",
    "#eta_matrix[:,72]\n",
    "print(eta_matrix.shape,'\\n')\n",
    "print(lda_model.get_topic_terms(3),'\\n')\n",
    "print(lda_model.get_topic_terms(4),'\\n')\n",
    "\n",
    "print(lda_model.get_topic_terms(5),'\\n')\n",
    "print(eta_matrix[3][72],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d6da79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c75796ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/LDA_model_on_proxy_data/\n"
     ]
    }
   ],
   "source": [
    "#################################\n",
    "#    Save the model\n",
    "#################################\n",
    "#homepath = '/home/ec2-user/SageMaker/'\n",
    "homepath = os.getcwd()\n",
    "model_path = homepath + 'LDA_model_on_proxy_data/'\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2a0eb9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to model_path and get list of componenets\n",
    "components = save_lda_model(lda_model, 'lda_model_on_proxy_data', model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ec1e4103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lda_model_on_proxy_data.model.id2word', 'lda_model_on_proxy_data.model', 'lda_model_on_proxy_data.model.expElogbeta.npy', '.ipynb_checkpoints', 'lda_model_on_proxy_data.model.state']\n"
     ]
    }
   ],
   "source": [
    "# Check if it works well\n",
    "print(os.listdir(model_path),'\\n')\n",
    "print(components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106d9879",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd46234",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "#    Upload to S3\n",
    "#################################\n",
    "bucket_name = 'lda-model-on-proxy-data'\n",
    "#homepath = '/home/ec2-user/SageMaker/'\n",
    "#model_path = homepath + 'LDA_model_on_proxy_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "8219a3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "# Upload eta_matrix and list of component to S3\n",
    "file_upload_helper(file = eta_matrix, file_name ='eta_matrix', bucket_name='lda-model-on-proxy-data')\n",
    "file_upload_helper(file = components, file_name ='components', bucket_name='lda-model-on-proxy-data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "1a737b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload model to S3\n",
    "model_upload_helper(components, model_path, bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bd0889",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf44674",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f4f0ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677f5042",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02916e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
