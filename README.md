# dissertation_project
dissertation project: auto e-WoM analysis system
# 1	data_collection
1.1	“tweepystreaming.py”
This script constantly pulls real-time tweets through Twitter streaming API.

# 2	model_1_on_proxy_data
2.1	“LDA_model_on_proxy_data.py”
This script first builds a basic LDA model on the proxy data and then tunes the model to determine proper number of topics. After building the optimal model, it will produce an eta matrix to estimate the probability of individual word belonging to each topic as prior understandings of the cell phone review data.

# 3	model_2
3.1	“Guided_LDA_model_on_pre_and_post_event_data.py”
This script builds a guided LDA model with the eta matrix produced in the script 2.1 and tunes the model on pre collected real-world data to determine the proper number of topics and hyperparameter settings in the current problem space. After tuning, the optimal model will be put into production environment, while a new eta matrix will be generated for continuous training

# 4	lambda_functions
4.1	/text_processing
4.1.1	“processingforSA-lambda.py”
This script periodically collects raw tweets data from the S3 bucket, cleans up their texts, and saves them in a new S3 bucket in preparation for sentiment analysis.

4.1.2	“processingforTopic-lambda.py”
This script periodically collects raw tweets data from the S3 bucket, cleans up to texts and conducts some initial text processing like tokenising, deleting stopwords and stemming. Then it saves processed data in a new S3 bucket in preparation for topic modelling.

4.2	/data_labelling
4.2.1	“sentiment-labelling-lambda.py”
This script periodically collects well-cleaned and processed tweet data and uses Textblob to mark sentimental polarity and subjectivity scores of its texts. The labelled data will be stored in a new S3 bucket.


4.2.2	“topic-labelling-lambda.py”
This script periodically collects well-cleaned and processed tweet data and pass them through the production LDA model to infer topic distributions and label top n topics in each tweet text. The labelled data will be stored in a new S3 bucket.

4.3	/saving_records
4.3.1	“matching_records_inserting_database-lambda.py”
This script periodically collects data labelled with sentiments and topics from different S3 bucket and match the records based on tweet_id. The merged data will be then inserted into the AWS RDS Mysql database.

4.4	/continuous_training
4.4.1	“monthly_continuous_training-lambda.py”
Triggered by scheduled event, this script trains and tunes a new LDA model with data collected in a full month and the eta matrix generated by the current production model. The new model and the training reports will be saved in a S3 bucket, waiting for manual deployment.

# 5	AWS_RDS
5.1	“creating_database_and_table_MySQL.sql”
This SQL script creates a new database and table for storing the tweets data labelled with both sentiments and topics, facilitating querying through a dashboard.

5.2	“checking.sql”
This SQL script helps to check validity of proposed system through querying real-time tweets records.
